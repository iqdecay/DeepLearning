{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "18cde020-8365-4923-8590-7d111b4a41f8"
    }
   },
   "source": [
    "# ELU 502 Deep learning  IMT Atlantique -- Lab session 6\n",
    "Pierre Tandeo, François Rousseau - session: 1h20+3h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "797413a2-61f3-40e6-a8f0-242b28d4e090"
    }
   },
   "source": [
    "### Objectives: Perform sentiment analysis classification on *IMDB dataset* exploring multiples architectures of recurrent neural networks (RNN, LSTM, BiLSTM, ...) and transfer learning from word embeddings vectors to achieve better results on a NLP (Natural Language Processing) task.\n",
    "\n",
    "Hint: See the documentation of Keras for implementation details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is IMDB ?**\n",
    "\n",
    "- \"Internet Movie DataBase\"\n",
    "- website that contains several movies reviews from users\n",
    "- [Imdb's site](https://www.imdb.com)\n",
    "\n",
    "Considering that we have only two classes of reviews (positive or negative). Could you predict the labels of reviews based only on text content of these reviews? Below, you will find 3 examples of reviews.\n",
    "\n",
    "**Review \\#1**\n",
    "\n",
    "*Quite simply, the finest gangster film ever made. No doubt about it, this a spectacular viewing experience. The acting along with the storyline makes this film a genuine masterpiece. The film covers a wide spectrum of genre keeping the viewer entertained throughout. TOP CLASS!*\n",
    "\n",
    "**Review \\#2**\n",
    "\n",
    "*This movie is like football, people only watch it because the think its 'cool' or 'popular', but really it is the most dreary repetitive and slow film i have had the displeasure in viewing. How this film grew to be such a ''great'' i do not know, bu ti know that it shouldn't be, Shawshank Redemption may not deserve to be second best film ever, but at least it is worthy of being where it is and is certainly a better film than any of the Godfathers.\n",
    "So please consider this when voting, the Godfather is poorly directed, badly scripted, crudely acted and most importantly is, quite frankly boring and wearisome.*\n",
    "\n",
    "**Review \\#3**\n",
    "\n",
    "*Great Book... Slightly above average movie (so sad)\n",
    "The book is awesome, one of the best books ever. Sadly the movie is weak, and fails to portrait the true essence of the characters, what's so great about the book is that you're able to know and understand the background stories, therefore you identify with the characters behavior to the point where you even forget they're mobsters!! however in the movie many minor and irrelevant scenes take too much time, time that could be used to go a little deeper into the characters, and what's even worst is the total disregard to important passages of the book which didn't even make the movie, or if they did, they were over-synthesized. I don't think I'm the only one that really loved the book, but was sadly disappointed by the movie, still is a movie worth watching, but if you really want to enjoy a great story you should read the book!!*\n",
    "\n",
    "\n",
    "Sometimes, even for us it is complicated to perform this task. In this lab session, we will create an algorithm that does these predictions automatically with more than 82.0% of accuracy. The state-of-the art methods achieves on this task (about 87.0% to 90.0%). Examples of application of this task are human dialogue sentiment identification, user opinion of product reviews, chat bots language understanding and/or Twitter language understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ec0f8430-76a1-475d-a360-8f93ab4f2732"
    }
   },
   "source": [
    "First, we need to download and import the IMDB dataset from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "2b07a011-1503-4935-8cb4-30013804a57c"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(path=\"imdb.npz\",\n",
    "                                                      num_words=None, # Number of words of the vocabulary (None=gets all words)\n",
    "                                                      skip_top=0,     # Excludes top-k frequent words\n",
    "                                                      maxlen=None,)   # Max length of sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ef32a37b-7b4d-48b6-af65-15f6d4fc2f11"
    }
   },
   "source": [
    "The text contains only discrete information:\n",
    "\n",
    "- each unique word is mapped to an unique index (integer)\n",
    "- each sentence or document contains a sequence of words (or indexes)\n",
    "\n",
    "In the dataset loaded by Keras the preprocessing is already done: words are already tokenized and indexed. The second step is to obtain the dictionary that translates indexes (integer) to words (string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "51ce5a95-c7f7-4a4f-aac8-f63cccd10c47"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Index of word \"special\" is 315\n",
      "> Index of word \"effects\" is 299\n"
     ]
    }
   ],
   "source": [
    "word_index = imdb.get_word_index(path='imdb_word_index.json') # Load word vocabulary dictionary \n",
    "print('> Index of word \\\"special\\\" is {}'.format(word_index['special']))\n",
    "print('> Index of word \\\"effects\\\" is {}'.format(word_index['effects']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1e3b1492-da1f-4c1b-82f2-97820ad210a1"
    }
   },
   "source": [
    "The reversed dictionary of *word_index* is obtained by the follow command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "177952ae-d1ac-45d1-81fa-20361b8a48cf"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> The word corresponding to the Index 315 is \"special\"\n",
      "> The word corresponding to the Index 299 is \"effects\"\n"
     ]
    }
   ],
   "source": [
    "index_word = dict([[v,k] for k,v in word_index.items()])\n",
    "print('> The word corresponding to the Index 315 is \\\"{}\\\"'.format(index_word[315]))\n",
    "print('> The word corresponding to the Index 299 is \\\"{}\\\"'.format(index_word[299]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "12084ea3-801c-46ca-a9c8-2442457d921f"
    }
   },
   "source": [
    "The number of unique words in the vocabulary is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "8eadbdf7-06fd-4d7c-bb63-fbfd1d7d4877"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Number of words in vocabulary: 88584\n"
     ]
    }
   ],
   "source": [
    "print('> Number of words in vocabulary: {}'.format(len(word_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5fae17c5-d856-45ea-b6c3-d01dd6dc501f"
    }
   },
   "source": [
    "## Part 1) Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "1c8c1351-b90b-4481-90d2-4decd3b8ad08"
    }
   },
   "outputs": [],
   "source": [
    "# Transform sequences of indexes to raw text\n",
    "# If the index is not in the vocabulary you need to continue without this index\n",
    "def indexes_to_text(indexes):\n",
    "    result = \"\"\n",
    "    for i in indexes:\n",
    "        if i not in index_word:\n",
    "            continue\n",
    "        try:\n",
    "            result = result+\" \"+str(index_word[i])\n",
    "        except:\n",
    "            continue\n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "418f9517-920a-4a9b-b2c2-89a3ac50107b"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Number of training examples (x_train): (25000,) \n",
      "\n",
      "> Number of training labeled examples (y_train): (25000,) \n",
      "\n",
      "> First training example (indexes) is: [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32] \n",
      "\n",
      "> First training example (raw text) is: \" the as you with out themselves powerful lets loves their becomes reaching had journalist of lot from anyone to have after out atmosphere never more room titillate it so heart shows to years of every never going villaronga help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but pratfalls to story wonderful that in seeing in character to of 70s musicians with heart had shadows they of here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other tricky in of seen over landed for anyone of gilmore's br show's to whether from than out themselves history he name half some br of 'n odd was two most of mean for 1 any an boat she he should is thought frog but of script you not while history he heart to real at barrel but when from one bit then have two of script their with her nobody most that with wasn't to with armed acting watch an for with heartfelt film want an\" \n",
      "\n",
      "> Label of first example is: \"1\" (positive = \"1\" and negative = \"0\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('> Number of training examples (x_train): {} \\n'.format(x_train.shape))\n",
    "print('> Number of training labeled examples (y_train): {} \\n'.format(y_train.shape))\n",
    "print('> First training example (indexes) is: {} \\n'.format(x_train[0]))\n",
    "print('> First training example (raw text) is: \\\"{}\\\" \\n'.format(indexes_to_text(x_train[0])))\n",
    "print('> Label of first example is: \\\"{}\\\" (positive = \\\"1\\\" and negative = \\\"0\\\")\\n'.format(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.1)** Find the first document review (training example) that appears the words *soundtrack* and *effects*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1236  the not was tom plot than taken in can as on it's good moments taken some br of entertaining believable series is far mistakes i i of you gave this so charismatic timing too regarded worse trying those in be enormously watch plot actors as example lungs br dressed to of script kill it ww who be iran all it jamie has seeing tries in giving in made to us found all begin nah film made just music even guaranteed which be mates florida for would story michel one chuck about terrific in illusion film regarded worse is over anti which be around mates tries is felt br made whom just by br though producer private tie rating know us four power tries scant fillmore to just scant fillmore regarded worse it last not is very set annoyed agrees interested this now acting play just louis comedy of sheriff attempt also message military streets br play movie version movie stagecoach not this as until on if of because too however stagecoach it boy damon has regarded worse not all against laugh read louis effects be relationship dietrich get movies name this be characters to that it theatre spin lifetime intense it yet br virgin all interesting virgin movie is courage fred not girl most br of changed really else thing danny made stagecoach it tony movie your not would effects is lasts to as utilizing are ross it by br of german words film of you of watched films he an good went br leader some br do it's life that an new characters to 8 introduction regarded it of relationship walk like trying those in kill in also an of next to all against laugh read has be you'll really commitment average polarized complexity intense it is ned not crying courage fred stagecoach it of various not time falling capote satires passionate did cut it is commune complaining quirky satires none it of violent feel like william first be grayson casting to there is france costumes to her are dressed i i of their places is stars br logo really he is over movies anything dozens but only twists regarded out involved in borg first be mates this is fighters be mates out involved in at chips tv or about facing no of extreme declares be covers but of how my to them more it of tell perhaps once i i this that is originally feel palance are romance regarded worse crime recently in same made this is locales br an details of put it fair plotted film of famous videos to trees america if likable this of vehicle to worst of feel crime talking regarded this artists br never originally movie is cadavers to guilt they made not lead like all language stagecoach would blair this match br of kidding in looked regarded is collection really earth ending forward 2 implausible they as fair surprisingly show earlier to sad once waste hit wonder think which north fear self to as involved are of every hard for stagecoach to louis can't william of violence br as lies horror in one you're broadway underdog introduction be script acting don't fan it of you'll br as on in at creating more it which is courage b this really is resurrection classical film regarded intense manage good collection ross some br of slapped odds flying that with just is imagination boys escape funny various 10 it of too louis with plus has of rest commune referenced capote to around get mitchell stagecoach movie of on hung bad same some for louis talking stagecoach in at movies various but made out movie of passionate capote talking louis in at movies along silly but about more it is soundtrack ok",
      " to it´s in at preying this of guy greater stagecoach stagecoach it cuts movie ambitious who seeing go of passionate capote it cuts movie is occasional to so settings of arrives nor it for of munsters being trying better movie girl various to his they an movie settings about baldwin avoid to civilization he episodes china aren't difference dealing to infected they there's pero being knew for would stagecoach it various more it funny storm for was nothing knowledge in laughed louis me ultimately beat so once but be crash their get just of violence br as on to series such all moments she film of blind broadway ann i i of you knows there is impressive down to of impression it mind not of guy old run believable parent in table are aren't lies john br are across fans films to songs i i what poor comment this leaves only arabia br improve stagecoach to because until bunker pack we who incredibly ensues\n"
     ]
    }
   ],
   "source": [
    "index_soundtrack = word_index[\"soundtrack\"]\n",
    "index_effects = word_index[\"effects\"]\n",
    "for k, v in enumerate(x_train):\n",
    "    if index_soundtrack in v and index_effects in v :\n",
    "        print(k, indexes_to_text(v))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2)** Find the minimum, mean and maximum number of words per review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max count : 2494 Min count : 11 Mean count : 238.71364\n",
      "> Number of classes: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_word_count = len(x_train[0])\n",
    "max_word_count = len(x_train[0])\n",
    "word_count_sum = 0\n",
    "for review in x_train :\n",
    "    l = len(review)\n",
    "    if min_word_count > l :\n",
    "        min_word_count = l\n",
    "    if max_word_count < l :\n",
    "        max_word_count = l\n",
    "    word_count_sum += l\n",
    "print(\"Max count : {} Min count : {} Mean count : {}\".format(max_word_count, min_word_count, word_count_sum/len(x_train)))\n",
    "# Shows the total number of labels\n",
    "num_classes = np.max(y_train)+1\n",
    "print('> Number of classes: {}\\n'.format(num_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.3)** What is the size of the training and test datasets before and after [padding](http://dmlc.ml/rstats/2017/10/11/rnn-bucket-mxnet-R.html)? Print the first training example with and without padding. Why do we use padding? What happens if the sequence length is shorter or longer than *maxlen*? Write your response in a new text cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing sequence data...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We select only sequences that contain less or equal than maxlen words\n",
    "maxlen = 500\n",
    "\n",
    "# We use all words of vocabulary\n",
    "max_words = len(word_index)\n",
    "print('Vectorizing sequence data...\\n')\n",
    "# Adding padding in sequences with less than max_words\n",
    "X_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the sequence length is short than maxlen, then, according to the documentation, the sequence is padded \n",
    "with the value 0.0 from the beginning (the maxlen-len(sequence) first words of the sequence are set to 0.0).\n",
    "As for sequence which length is longer than maxlen, then only the last maxlen words of the sequence are kept.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2) Many-to-one sequence logistic classifier\n",
    "\n",
    "The image above shows a simple RNN model that for each time $1 \\leq t \\leq t_{max}$ gives the output vector $o_t$ based on inputs $s_{t-1}$ and $x_t$.\n",
    "\n",
    "In a many-to-one sequence logistic classifier, the final output is $o_{t_{max}}$. In the case of predicting labels for reviews the last output vector is used in the logistic regression. We call in lab session the vector $s_{t_{max}}$ as context vector.\n",
    "\n",
    "We could summarize the RNN model as a function:\n",
    "\n",
    "$$ f_{RNN}(U, V, W; s_0, x_1, x_2, ... x_{t_{max}}) = o_{t_{max}}  = P(y=1 | s_0, x_1, x_2, ..., x_{t_{max}})$$\n",
    "\n",
    "The RNN model is optimized following the binary cross-entropy criteria. For the example $i$:\n",
    "\n",
    "$$L_{i}= -[y_i log  (o_{t_{max}}) + (1-y_i) log (1-o_{t_{max}})]$$\n",
    "\n",
    "The optimization algorithm consists in finding a good solution the equation above on the set of training samples:\n",
    "\n",
    "$$ arg_{\\theta} min \\frac{1}{N}\\sum_{i}^{N}L_{i}$$\n",
    "\n",
    "![RNN model](http://www.wildml.com/wp-content/uploads/2015/09/rnn.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we also introduce the notion of [embedding](http://dmlc.ml/rstats/2017/10/11/rnn-bucket-mxnet-R.html). Embedding turns positive integers (indexes) into dense vectors of fixed size. eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]. In this last example, we transform the sequence of indexes 4 and 20 into real vectors respectively (0.25, 0.1) and (0.6, -0.2). The idea of embedding is to project the indices into another space that is more informative. For instance, words with similar meaning will be close in this new space.\n",
    "\n",
    "In Keras, we can add an [embedding layer](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/) in a RNN. In that case, all word vectors (for instance [0.25, 0.1] and [0.6, -0.2]) are now parameters of the RNN and they are optimized as parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Simple RNN model with Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we give you an example of simple RNN declaration with an embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 300)          26575200  \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 50)                17550     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 51        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 26,592,801\n",
      "Trainable params: 26,592,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import SimpleRNN, Dense, Embedding, Activation\n",
    "from keras.models import Sequential\n",
    "\n",
    "embedding_dims = 300\n",
    "rnn_units = 50\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# max_words is the vocabulary size\n",
    "# embeddings_dim is the dimension of each word embeddings vector\n",
    "# maxlen is the maximum length of each sequence\n",
    "\n",
    "# Embedding layer definition\n",
    "model.add(Embedding(max_words, embedding_dims, input_length=maxlen))\n",
    "# Recurrent layer definition\n",
    "model.add(SimpleRNN(rnn_units, activation='tanh', return_sequences=False)) # Output of this layer is the context vector\n",
    "# Linear layer definition\n",
    "model.add(Dense(num_classes-1))\n",
    "# Non-linear function bounded (0 to 1)\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "# Show the model architecture\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 146s 6ms/step - loss: 0.4895 - acc: 0.7634 - val_loss: 0.4137 - val_acc: 0.8267\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 72s 3ms/step - loss: 0.2008 - acc: 0.9242 - val_loss: 0.4028 - val_acc: 0.8308\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 66s 3ms/step - loss: 0.0513 - acc: 0.9847 - val_loss: 0.4483 - val_acc: 0.8440\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 66s 3ms/step - loss: 0.0109 - acc: 0.9981 - val_loss: 0.5324 - val_acc: 0.8173\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 67s 3ms/step - loss: 0.0032 - acc: 0.9997 - val_loss: 0.5582 - val_acc: 0.8388\n",
      "25000/25000 [==============================] - 22s 869us/step\n",
      "Test loss: 0.5582\n",
      "Test Accuracy: 0.8388\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "batch_size=128\n",
    "num_epochs=5\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Evaluate model\n",
    "test_loss, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "    \n",
    "print('Test loss: %1.4f' % test_loss)\n",
    "print('Test Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) LSTM/BiLSTM model with Embedding Layer\n",
    "\n",
    "\n",
    "The LSTM model is more complex than the RNN model. It is pictured in the figure above:\n",
    "\n",
    "![LSTM model](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.1)** What are the advantages in using LSTM instead of a simple RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important information can be located a the very beginning of a sequence. For instance in predicting which words comes next in the sentence \"John came in the room and [...]. As she left the room, Sabrina said goodbye to ??? \", the important information is the first one. Simple RNN don't take into account the first information, or rather they take it into account so little that it can be negliged.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2)** What is a bidirectional recurrent model? What kind of operation is performed in context vectors of each direction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bidirectionnal recurrent model tries to also match the input given an output, as a way to have a better training process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3)** Write your own LSTM and/or Bidirectional LSTM model and compare the results with the simple RNN model (use *CuDNNLSTM* layer instead of *LSTM* for faster results). Plot the performance rate of your own model as a function of epochs and try to avoid the problem of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.layers import CuDNNLSTM \n",
    "embedding_dims = 300\n",
    "rnn_units = 50\n",
    "\n",
    "print('Build model...')\n",
    "our_model = Sequential()\n",
    "\n",
    "# max_words is the vocabulary size\n",
    "# embeddings_dim is the dimension of each word embeddings vector\n",
    "# maxlen is the maximum length of each sequence\n",
    "\n",
    "# Embedding layer definition\n",
    "our_model.add(Embedding(max_words, embedding_dims, input_length=maxlen))\n",
    "# Recurrent layer definition\n",
    "our_model.add(CuDNNLSTM(num_classes-1)) # Output of this layer is the context vector\n",
    "# Linear layer definition\n",
    "our_model.add(Dropout(0.3))\n",
    "our_model.add(Dense(num_classes-1))\n",
    "our_model.add(Dropout(0.3))\n",
    "# Non-linear function bounded (0 to 1)\n",
    "our_model.add(Activation('sigmoid'))\n",
    "\n",
    "our_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "# Show the model architecture\n",
    "print (our_model.summary())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "batch_size=128\n",
    "num_epochs=5\n",
    "h = our_model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs, validation_data=(X_test, y_test), verbose=1)\n",
    "acc = []\n",
    "for i in h.history['val_acc'] :\n",
    "    acc.append(i)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot([i for i in range(num_epochs)], acc)\n",
    "plt.show()\n",
    "# Evaluate model\n",
    "test_loss, acc = our_model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "    \n",
    "print('Test loss: %1.4f' % test_loss)\n",
    "print('Test Accuracy: %1.4f' % acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we had an issue with overfitting : the difference (train_accuracy - test_accuracy) was a growing function of the number of epochs, with a very high (99%) train accuracy. Adding dropout layers significantly decreased train_accuracy, but also fixed the overfitting issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3) Analysis of results and data visualization \n",
    "### 3.1) t-SNE 2D Visualization of context vectors (test data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-distributed Stochastic Neighbor Embedding (t-SNE)[1] is a non-supervised technique of visualization of high-dimensional data based on KL divergence probabilities approximation to construct a [manifold](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) (see [1] Maaten, Laurens van der, and Geoffrey Hinton. \"Visualizing data using t-SNE.\" Journal of machine learning research 9, no. Nov (2008): 2579-2605).\n",
    "\n",
    "In this example, we will use this technique to visualize context vectors of each document (review) in a 2D space. The original euclidian distance between high dimensional context vectors is mostly preserved. Therefore, we can have an idea of the projection of context vectors in a 2D space. \n",
    "\n",
    "We will plot this representation using a [scatter plot](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.scatter.html) (please use a sub-sampling of test data for visualization, *n_subsamples* parameter defined below). The colors of the points will represent the real label (positive or negative review) of the predicted context vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import math\n",
    "from keras import backend as K\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function that takes the output of a specific layer of the model\n",
    "def get_activations(model, id_layer, X_batch):\n",
    "    get_activations = K.function([model.layers[0].input, K.learning_phase()], [model.layers[id_layer].output])\n",
    "    activations = get_activations([X_batch,0])\n",
    "    return activations\n",
    "\n",
    "#  Function that takes the output of a rnn layer of the model (use batch)\n",
    "def get_context_vectors(model, X, id_rnn_layer = 1):\n",
    "    batch_size_test = 1024\n",
    "    h_context = get_activations(model, id_rnn_layer, X[0,:].reshape(1,-1))[0]\n",
    "    divs = math.floor(X.shape[0]/batch_size_test)\n",
    "    h_ = np.zeros((X.shape[0], rnn_units))\n",
    "    for i in range(int(divs)):\n",
    "        h_context = get_activations(model, id_rnn_layer, X_test[i*batch_size_test:(i+1)*batch_size_test,:].reshape(batch_size_test,-1))[0]\n",
    "        h_[i*batch_size_test:(i+1)*batch_size_test,:] = h_context.reshape(-1, rnn_units)\n",
    "    if X.shape[0] > divs*batch_size_test:\n",
    "        first_id = int(divs*batch_size_test)\n",
    "        h_context = get_activations(model,  id_rnn_layer, X[first_id :,:])[0]\n",
    "        h_[first_id :,:] = h_context.reshape(-1, rnn_units)\n",
    "    h_ = np.asarray(h_).reshape(-1, rnn_units)\n",
    "    return h_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1)** Get context vectors (for training and test data) and complete the tsne transformation of these vectors and then plot using scatter function of matplotlib with a colorbar (Hint: use *get_context_vectors* function and specify correctly the id of the layer). Explain the obtained results in a new text cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only plot a sub-sampling of the dataset\n",
    "n_subsamples = 1000\n",
    "\n",
    "# Please get context vectors and complete the tsne transformation of these vectors\n",
    "context_vector_train = get_context_vectors(our_model, X_train,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) Show output probabilities (sigmoid output) for each word of a random review on test data\n",
    "\n",
    "The objective of this subsection is getting predictions of each word added in the sequence. This type of analysis increases the interpretability of the RNN decision, to identify when the model changes the decision for instance.\n",
    "\n",
    "In the next cell, we:\n",
    "- Get a random review on test data with less than 25 words (count the number of non-zeros entries because padding is indexed by 0 element)\n",
    "- Iterate until find first random review with less than 25 words\n",
    "- Converts the numpy sample in a list of integers without padding\n",
    "- Show the raw text of the review (use function *indexes_to_text*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "max_words_constraint=30\n",
    "non_zeros_indexes=X_test.shape[1]\n",
    "\n",
    "# Get a random review with less than max_words_contraint words\n",
    "while non_zeros_indexes > max_words_constraint:\n",
    "    id_random_review = random.randint(0, int(X_test.shape[0]))\n",
    "    non_zeros_indexes = len(np.nonzero(X_test[int(id_random_review),:])[0])\n",
    "    \n",
    "# Create a list of non-zeros elements for remove padding\n",
    "random_review_sample = list()\n",
    "for i in X_test[id_random_review,:].reshape(-1).tolist():\n",
    "    if i != 0:\n",
    "        random_review_sample.append(i)\n",
    "print(indexes_to_text(random_review_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.2)** Here, we ask you to:\n",
    "- Add words progressively (get one word per time and creates a new sub-sequence containing past words)\n",
    "- Create sub-sequences of words and add padding for each one of these sub-sequences (Hint: use function *sequence.pad_sequences(seq, maxlen=maxlen)*)\n",
    "- Get the sigmoid output and print it (Hint: use *function get_activations(model, id_layer, X_batch)* on each subsequence)\n",
    "- Create a function *get_predictions* and print the conditional probability of positive prediction given the subsequence\n",
    "- Create a plot with that shows the evolution of predictions (a time series) during the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3) Create your own movie review and test it with predictions of your trained model\n",
    "\n",
    "**Question 3.3)** Now, it is you turn to:\n",
    "- Generate your own reviews (both bads and goods)\n",
    "- Lowercase all words of sentence (Hint: *.lower()*)\n",
    "- Use a tokenizer or a simple split for separate words (Hint: *.split(' ')*)\n",
    "- Excludes words that are not in vocabulary\n",
    "- Replace words by indexes\n",
    "- Get the prediction result for each word of your review (Hint: use the function created in the last exercice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
