{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "18cde020-8365-4923-8590-7d111b4a41f8"
    }
   },
   "source": [
    "# ELU 502 Deep learning  IMT Atlantique -- Lab session 6\n",
    "Pierre Tandeo, FranÃ§ois Rousseau - session: 1h20+3h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "797413a2-61f3-40e6-a8f0-242b28d4e090"
    }
   },
   "source": [
    "### Objectives: Perform sentiment analysis classification on *IMDB dataset* exploring multiples architectures of recurrent neural networks (RNN, LSTM, BiLSTM, ...) and transfer learning from word embeddings vectors to achieve better results on a NLP (Natural Language Processing) task.\n",
    "\n",
    "Hint: See the documentation of Keras for implementation details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is IMDB ?**\n",
    "\n",
    "- \"Internet Movie DataBase\"\n",
    "- website that contains several movies reviews from users\n",
    "- [Imdb's site](https://www.imdb.com)\n",
    "\n",
    "Considering that we have only two classes of reviews (positive or negative). Could you predict the labels of reviews based only on text content of these reviews? Below, you will find 3 examples of reviews.\n",
    "\n",
    "**Review \\#1**\n",
    "\n",
    "*Quite simply, the finest gangster film ever made. No doubt about it, this a spectacular viewing experience. The acting along with the storyline makes this film a genuine masterpiece. The film covers a wide spectrum of genre keeping the viewer entertained throughout. TOP CLASS!*\n",
    "\n",
    "**Review \\#2**\n",
    "\n",
    "*This movie is like football, people only watch it because the think its 'cool' or 'popular', but really it is the most dreary repetitive and slow film i have had the displeasure in viewing. How this film grew to be such a ''great'' i do not know, bu ti know that it shouldn't be, Shawshank Redemption may not deserve to be second best film ever, but at least it is worthy of being where it is and is certainly a better film than any of the Godfathers.\n",
    "So please consider this when voting, the Godfather is poorly directed, badly scripted, crudely acted and most importantly is, quite frankly boring and wearisome.*\n",
    "\n",
    "**Review \\#3**\n",
    "\n",
    "*Great Book... Slightly above average movie (so sad)\n",
    "The book is awesome, one of the best books ever. Sadly the movie is weak, and fails to portrait the true essence of the characters, what's so great about the book is that you're able to know and understand the background stories, therefore you identify with the characters behavior to the point where you even forget they're mobsters!! however in the movie many minor and irrelevant scenes take too much time, time that could be used to go a little deeper into the characters, and what's even worst is the total disregard to important passages of the book which didn't even make the movie, or if they did, they were over-synthesized. I don't think I'm the only one that really loved the book, but was sadly disappointed by the movie, still is a movie worth watching, but if you really want to enjoy a great story you should read the book!!*\n",
    "\n",
    "\n",
    "Sometimes, even for us it is complicated to perform this task. In this lab session, we will create an algorithm that does these predictions automatically with more than 82.0% of accuracy. The state-of-the art methods achieves on this task (about 87.0% to 90.0%). Examples of application of this task are human dialogue sentiment identification, user opinion of product reviews, chat bots language understanding and/or Twitter language understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ec0f8430-76a1-475d-a360-8f93ab4f2732"
    }
   },
   "source": [
    "First, we need to download and import the IMDB dataset from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "2b07a011-1503-4935-8cb4-30013804a57c"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(path=\"imdb.npz\",\n",
    "                                                      num_words=None, # Number of words of the vocabulary (None=gets all words)\n",
    "                                                      skip_top=0,     # Excludes top-k frequent words\n",
    "                                                      maxlen=None,)   # Max length of sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ef32a37b-7b4d-48b6-af65-15f6d4fc2f11"
    }
   },
   "source": [
    "The text contains only discrete information:\n",
    "\n",
    "- each unique word is mapped to an unique index (integer)\n",
    "- each sentence or document contains a sequence of words (or indexes)\n",
    "\n",
    "In the dataset loaded by Keras the preprocessing is already done: words are already tokenized and indexed. The second step is to obtain the dictionary that translates indexes (integer) to words (string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "51ce5a95-c7f7-4a4f-aac8-f63cccd10c47"
    }
   },
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index(path='imdb_word_index.json') # Load word vocabulary dictionary \n",
    "print('> Index of word \\\"special\\\" is {}'.format(word_index['special']))\n",
    "print('> Index of word \\\"effects\\\" is {}'.format(word_index['effects']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1e3b1492-da1f-4c1b-82f2-97820ad210a1"
    }
   },
   "source": [
    "The reversed dictionary of *word_index* is obtained by the follow command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "177952ae-d1ac-45d1-81fa-20361b8a48cf"
    }
   },
   "outputs": [],
   "source": [
    "index_word = dict([[v,k] for k,v in word_index.items()])\n",
    "print('> The word corresponding to the Index 315 is \\\"{}\\\"'.format(index_word[315]))\n",
    "print('> The word corresponding to the Index 299 is \\\"{}\\\"'.format(index_word[299]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "12084ea3-801c-46ca-a9c8-2442457d921f"
    }
   },
   "source": [
    "The number of unique words in the vocabulary is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "8eadbdf7-06fd-4d7c-bb63-fbfd1d7d4877"
    }
   },
   "outputs": [],
   "source": [
    "print('> Number of words in vocabulary: {}'.format(len(word_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5fae17c5-d856-45ea-b6c3-d01dd6dc501f"
    }
   },
   "source": [
    "## Part 1) Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "1c8c1351-b90b-4481-90d2-4decd3b8ad08"
    }
   },
   "outputs": [],
   "source": [
    "# Transform sequences of indexes to raw text\n",
    "# If the index is not in the vocabulary you need to continue without this index\n",
    "def indexes_to_text(indexes):\n",
    "    result = \"\"\n",
    "    for i in indexes:\n",
    "        if i not in index_word:\n",
    "            continue\n",
    "        try:\n",
    "            result = result+\" \"+str(index_word[i])\n",
    "        except:\n",
    "            continue\n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "418f9517-920a-4a9b-b2c2-89a3ac50107b"
    }
   },
   "outputs": [],
   "source": [
    "print('> Number of training examples (x_train): {} \\n'.format(x_train.shape))\n",
    "print('> Number of training labeled examples (y_train): {} \\n'.format(y_train.shape))\n",
    "print('> First training example (indexes) is: {} \\n'.format(x_train[0]))\n",
    "print('> First training example (raw text) is: \\\"{}\\\" \\n'.format(indexes_to_text(x_train[0])))\n",
    "print('> Label of first example is: \\\"{}\\\" (positive = \\\"1\\\" and negative = \\\"0\\\")\\n'.format(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.1)** Find the first document review (training example) that appears the words *soundtrack* and *effects*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2)** Find the minimum, mean and maximum number of words per review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TO DO ###\n",
    "\n",
    "# Shows the total number of labels\n",
    "num_classes = np.max(y_train)+1\n",
    "print('> Number of classes: {}\\n'.format(num_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.3)** What is the size of the training and test datasets before and after [padding](http://dmlc.ml/rstats/2017/10/11/rnn-bucket-mxnet-R.html)? Print the first training example with and without padding. Why do we use padding? What happens if the sequence length is shorter or longer than *maxlen*? Write your response in a new text cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We select only sequences that contain less or equal than maxlen words\n",
    "maxlen = 500\n",
    "\n",
    "# We use all words of vocabulary\n",
    "max_words = len(word_index)\n",
    "print('Vectorizing sequence data...\\n')\n",
    "# Adding padding in sequences with less than max_words\n",
    "X_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2) Many-to-one sequence logistic classifier\n",
    "\n",
    "The image above shows a simple RNN model that for each time $1 \\leq t \\leq t_{max}$ gives the output vector $o_t$ based on inputs $s_{t-1}$ and $x_t$.\n",
    "\n",
    "In a many-to-one sequence logistic classifier, the final output is $o_{t_{max}}$. In the case of predicting labels for reviews the last output vector is used in the logistic regression. We call in lab session the vector $s_{t_{max}}$ as context vector.\n",
    "\n",
    "We could summarize the RNN model as a function:\n",
    "\n",
    "$$ f_{RNN}(U, V, W; s_0, x_1, x_2, ... x_{t_{max}}) = o_{t_{max}}  = P(y=1 | s_0, x_1, x_2, ..., x_{t_{max}})$$\n",
    "\n",
    "The RNN model is optimized following the binary cross-entropy criteria. For the example $i$:\n",
    "\n",
    "$$L_{i}= -[y_i log  (o_{t_{max}}) + (1-y_i) log (1-o_{t_{max}})]$$\n",
    "\n",
    "The optimization algorithm consists in finding a good solution the equation above on the set of training samples:\n",
    "\n",
    "$$ arg_{\\theta} min \\frac{1}{N}\\sum_{i}^{N}L_{i}$$\n",
    "\n",
    "![RNN model](http://www.wildml.com/wp-content/uploads/2015/09/rnn.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we also introduce the notion of [embedding](http://dmlc.ml/rstats/2017/10/11/rnn-bucket-mxnet-R.html). Embedding turns positive integers (indexes) into dense vectors of fixed size. eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]. In this last example, we transform the sequence of indexes 4 and 20 into real vectors respectively (0.25, 0.1) and (0.6, -0.2). The idea of embedding is to project the indices into another space that is more informative. For instance, words with similar meaning will be close in this new space.\n",
    "\n",
    "In Keras, we can add an [embedding layer](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/) in a RNN. In that case, all word vectors (for instance [0.25, 0.1] and [0.6, -0.2]) are now parameters of the RNN and they are optimized as parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Simple RNN model with Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we give you an example of simple RNN declaration with an embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN, Dense, Embedding, Activation\n",
    "from keras.models import Sequential\n",
    "\n",
    "embedding_dims = 300\n",
    "rnn_units = 50\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# max_words is the vocabulary size\n",
    "# embeddings_dim is the dimension of each word embeddings vector\n",
    "# maxlen is the maximum length of each sequence\n",
    "\n",
    "# Embedding layer definition\n",
    "model.add(Embedding(max_words, embedding_dims, input_length=maxlen))\n",
    "# Recurrent layer definition\n",
    "model.add(SimpleRNN(rnn_units, activation='tanh', return_sequences=False)) # Output of this layer is the context vector\n",
    "# Linear layer definition\n",
    "model.add(Dense(num_classes-1))\n",
    "# Non-linear function bounded (0 to 1)\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "# Show the model architecture\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "batch_size=128\n",
    "num_epochs=5\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs, validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "# Evaluate model\n",
    "test_loss, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "    \n",
    "print('Test loss: %1.4f' % test_loss)\n",
    "print('Test Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) LSTM/BiLSTM model with Embedding Layer\n",
    "\n",
    "\n",
    "The LSTM model is more complex than the RNN model. It is pictured in the figure above:\n",
    "\n",
    "![LSTM model](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.1)** What are the advantages in using LSTM instead of a simple RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2)** What is a bidirectional recurrent model? What kind of operation is performed in context vectors of each direction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3)** Write your own LSTM and/or Bidirectional LSTM model and compare the results with the simple RNN model (use *CuDNNLSTM* layer instead of *LSTM* for faster results). Plot the performance rate of your own model as a function of epochs and try to avoid the problem of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3) Analysis of results and data visualization \n",
    "### 3.1) t-SNE 2D Visualization of context vectors (test data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-distributed Stochastic Neighbor Embedding (t-SNE)[1] is a non-supervised technique of visualization of high-dimensional data based on KL divergence probabilities approximation to construct a [manifold](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) (see [1] Maaten, Laurens van der, and Geoffrey Hinton. \"Visualizing data using t-SNE.\" Journal of machine learning research 9, no. Nov (2008): 2579-2605).\n",
    "\n",
    "In this example, we will use this technique to visualize context vectors of each document (review) in a 2D space. The original euclidian distance between high dimensional context vectors is mostly preserved. Therefore, we can have an idea of the projection of context vectors in a 2D space. \n",
    "\n",
    "We will plot this representation using a [scatter plot](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.scatter.html) (please use a sub-sampling of test data for visualization, *n_subsamples* parameter defined below). The colors of the points will represent the real label (positive or negative review) of the predicted context vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import math\n",
    "from keras import backend as K\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function that takes the output of a specific layer of the model\n",
    "def get_activations(model, id_layer, X_batch):\n",
    "    get_activations = K.function([model.layers[0].input, K.learning_phase()], [model.layers[id_layer].output])\n",
    "    activations = get_activations([X_batch,0])\n",
    "    return activations\n",
    "\n",
    "#  Function that takes the output of a rnn layer of the model (use batch)\n",
    "def get_context_vectors(model, X, id_rnn_layer = 1):\n",
    "    batch_size_test = 1024\n",
    "    h_context = get_activations(model, id_rnn_layer, X[0,:].reshape(1,-1))[0]\n",
    "    divs = math.floor(X.shape[0]/batch_size_test)\n",
    "    h_ = np.zeros((X.shape[0], rnn_units))\n",
    "    for i in range(int(divs)):\n",
    "        h_context = get_activations(model, id_rnn_layer, X_test[i*batch_size_test:(i+1)*batch_size_test,:].reshape(batch_size_test,-1))[0]\n",
    "        h_[i*batch_size_test:(i+1)*batch_size_test,:] = h_context.reshape(-1, rnn_units)\n",
    "    if X.shape[0] > divs*batch_size_test:\n",
    "        first_id = int(divs*batch_size_test)\n",
    "        h_context = get_activations(model,  id_rnn_layer, X[first_id :,:])[0]\n",
    "        h_[first_id :,:] = h_context.reshape(-1, rnn_units)\n",
    "    h_ = np.asarray(h_).reshape(-1, rnn_units)\n",
    "    return h_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1)** Get context vectors (for training and test data) and complete the tsne transformation of these vectors and then plot using scatter function of matplotlib with a colorbar (Hint: use *get_context_vectors* function and specify correctly the id of the layer). Explain the obtained results in a new text cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only plot a sub-sampling of the dataset\n",
    "n_subsamples = 1000\n",
    "\n",
    "# Please get context vectors and complete the tsne transformation of these vectors\n",
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) Show output probabilities (sigmoid output) for each word of a random review on test data\n",
    "\n",
    "The objective of this subsection is getting predictions of each word added in the sequence. This type of analysis increases the interpretability of the RNN decision, to identify when the model changes the decision for instance.\n",
    "\n",
    "In the next cell, we:\n",
    "- Get a random review on test data with less than 25 words (count the number of non-zeros entries because padding is indexed by 0 element)\n",
    "- Iterate until find first random review with less than 25 words\n",
    "- Converts the numpy sample in a list of integers without padding\n",
    "- Show the raw text of the review (use function *indexes_to_text*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "max_words_constraint=30\n",
    "non_zeros_indexes=X_test.shape[1]\n",
    "\n",
    "# Get a random review with less than max_words_contraint words\n",
    "while non_zeros_indexes > max_words_constraint:\n",
    "    id_random_review = random.randint(0, int(X_test.shape[0]))\n",
    "    non_zeros_indexes = len(np.nonzero(X_test[int(id_random_review),:])[0])\n",
    "    \n",
    "# Create a list of non-zeros elements for remove padding\n",
    "random_review_sample = list()\n",
    "for i in X_test[id_random_review,:].reshape(-1).tolist():\n",
    "    if i != 0:\n",
    "        random_review_sample.append(i)\n",
    "print(indexes_to_text(random_review_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.2)** Here, we ask you to:\n",
    "- Add words progressively (get one word per time and creates a new sub-sequence containing past words)\n",
    "- Create sub-sequences of words and add padding for each one of these sub-sequences (Hint: use function *sequence.pad_sequences(seq, maxlen=maxlen)*)\n",
    "- Get the sigmoid output and print it (Hint: use *function get_activations(model, id_layer, X_batch)* on each subsequence)\n",
    "- Create a function *get_predictions* and print the conditional probability of positive prediction given the subsequence\n",
    "- Create a plot with that shows the evolution of predictions (a time series) during the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3) Create your own movie review and test it with predictions of your trained model\n",
    "\n",
    "**Question 3.3)** Now, it is you turn to:\n",
    "- Generate your own reviews (both bads and goods)\n",
    "- Lowercase all words of sentence (Hint: *.lower()*)\n",
    "- Use a tokenizer or a simple split for separate words (Hint: *.split(' ')*)\n",
    "- Excludes words that are not in vocabulary\n",
    "- Replace words by indexes\n",
    "- Get the prediction result for each word of your review (Hint: use the function created in the last exercice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
