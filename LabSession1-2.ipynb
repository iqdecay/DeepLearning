{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"image\">\n",
    "<img src=\"https://www.imt-atlantique.fr/sites/default/files/logo_mt_0_0.png\" WIDTH=280 HEIGHT=280>\n",
    "</div>\n",
    "<div id=\"subject\">\n",
    "<CENTER>\n",
    "</br>\n",
    "<font size=\"5\"></br> UE Deep Learning: Lab sessions 1 and 2</font></br></div>\n",
    "</CENTER>\n",
    "<CENTER>\n",
    "<font size=\"3\"></br>15th and 19th of Octobre 2018</font></br></div>\n",
    "</CENTER>\n",
    "<CENTER>\n",
    "<span style=\"color:blue\">pierre.tandeo@imt-atlantique.fr</span>\n",
    "</CENTER>\n",
    "<CENTER>\n",
    "<span style=\"color:blue\">pierre-henri.conze@imt-atlantique.fr</span>\n",
    "</CENTER>\n",
    "<CENTER>\n",
    "<span style=\"color:blue\">lucas.drumetz@imt-atlantique.fr</span>\n",
    "</CENTER>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this practice is to explain $Y$ (output variable) as a function of $X$ (input variable) using neural networks. Here, $X$ are continuous and $Y$ can be continuous (*regression*) or discrete (*classification*). These 2 aspects are treated in the deep learning framework. Both linear and nonlinear cases will be covered in this lab session.\n",
    "\n",
    "We use the Keras Python library to implement deep learning architectures. This interface has the following characteristics:\n",
    "- it is developped for both *Tensorflow* and *Theano*\n",
    "- it can be applied to CPU or GPU without distinction\n",
    "- it can interact with classical machine learning libraries like *Scikit-learn*\n",
    "\n",
    "This lab session is an introduction. It gives you the general concepts of neural nets and helps you to implement them using a dedicated Python library. If you want to play with similar regression/classification examples in small dimensions, you can use http://playground.tensorflow.org/. For more details concerning Keras, please visit the documentation at https://keras.io/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize']=(15,15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression problems**\n",
    "\n",
    "Simple regression is a linear problem between continuous variables $X$ and $Y$. Here, we write the model $Y=2+0.5X$ and generate $Y$ using an additional Gaussian standard random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "n=1000\n",
    "X=random.uniform(-10,10,n)\n",
    "Y_true=2+0.5*X # true model\n",
    "Y=Y_true+random.normal(0,1,n) # add noise to the truth\n",
    "plot(X,Y,'b.')\n",
    "\n",
    "# Plot noisy data and true model\n",
    "plot(X,Y,'b.')\n",
    "plot(X,Y_true,'r.')\n",
    "xlabel('X')\n",
    "ylabel('Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the objective is to build a neural net to fit the linear relationships between $X$ and $Y$. We use the mean absolute error as loss function and the stochastic gradient descent (sgd) algorithm for the optimization procedure. Then, we fit the model on a training dataset (80%) and we evaluate the performance of the model on a test dataset (20%). Finally, we plot the estimated parameters corresponding to the weights for each neurone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import deep learning library\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dense\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "# Build our model\n",
    "model_regress1 = Sequential()\n",
    " \n",
    "# Declare the layers\n",
    "layers = [Dense(units=1, input_dim=1), Activation('linear')]\n",
    " \n",
    "# Add the layers to the model\n",
    "for layer in layers:\n",
    "    model_regress1.add(layer)\n",
    "\n",
    "# Configure an optimizer used to minimize the loss function\n",
    "sgd = SGD(lr=0.1, decay=.01)\n",
    "\n",
    "# Compile our model\n",
    "model_regress1.compile(loss='mean_absolute_error', optimizer=sgd)\n",
    " \n",
    "# Fit the model\n",
    "model_regress1.fit(X, Y, validation_split=0.2, epochs=100)\n",
    "\n",
    "# Model summary and weights\n",
    "model_regress1.get_weights() # model_regress1.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the estimated parameters are closed to the true ones ($2$ for the intercept and $0.5$ for the slope). Below, we plot the true model (red) and the estimated one (green)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction using the model\n",
    "Y_hat = model_regress1.predict(X)\n",
    "\n",
    "# Plot noisy data, true model and prediction\n",
    "plot(X,Y,'b.')\n",
    "plot(X,Y_true,'r.',linewidth=2)\n",
    "plot(X,Y_hat,'g.',linewidth=2)\n",
    "xlabel('X')\n",
    "ylabel('Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us consider a model where for a same input variable $X$, we associate 2 output variables $Y_0$ and $Y_1$ defined by $Y_0=2+0.5X$ and $Y_1=-2-0.5X$. As previously, we use Gaussian additive noises to generate $Y_0$ and $Y_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "n=1000\n",
    "X=random.uniform(-10,10,n)\n",
    "Y0_true=2+0.5*X # true model\n",
    "Y0=Y0_true+random.normal(0,1,n) # add noise\n",
    "Y1_true=-2-0.5*X # true model\n",
    "Y1=Y1_true+random.normal(0,1,n) # add noise\n",
    "Y_true=vstack([Y0_true,Y1_true]).T\n",
    "Y=vstack([Y0,Y1]).T\n",
    "\n",
    "# Plot noisy data and true model\n",
    "plot(X,Y,'b.')\n",
    "plot(X,Y_true,'r.')\n",
    "xlabel('X')\n",
    "ylabel('Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we modify the hidden layer to account for the 2 output variables $Y_0$ et $Y_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build our model\n",
    "model_regress2 = Sequential()\n",
    "\n",
    "# Declare the layers\n",
    "layers = [Dense(units=2,input_dim=1), Activation('linear')]\n",
    "\n",
    "# Add the layers to the model\n",
    "for layer in layers:\n",
    "    model_regress2.add(layer)\n",
    "    \n",
    "# Configure an optimizer used to minimize the loss function\n",
    "sgd = SGD(lr=0.1, decay=.01)\n",
    "\n",
    "# Compile our model\n",
    "model_regress2.compile(loss='mean_absolute_error', optimizer=sgd)\n",
    " \n",
    "# Fit the model\n",
    "model_regress2.fit(X, Y, validation_split=0.2, epochs=100)\n",
    "                   \n",
    "# Model weights\n",
    "model_regress2.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compare the fitted (green) and true (red) models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction using the model\n",
    "Y_hat = model_regress2.predict(X)\n",
    "\n",
    "# Plot train, test and hat\n",
    "plot(X,Y,'b.')\n",
    "plot(X,Y_true,'.r',linewidth=2)\n",
    "plot(X,Y_hat,'.g',linewidth=2)\n",
    "xlabel('X')\n",
    "ylabel('Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification problems**\n",
    "\n",
    "In a classification problem, $Y$ is a discrete variable with various classes. Here, $Y$ is binary and takes its values between $0$ and $1$. We write the model as $Y=f\\left(2+0.5 X\\right)$ with $f$ the logistic transfer function (sigmoid). As previously we generate $Y$ adding Gaussian perturbations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "n=1000\n",
    "X=random.uniform(-14,8,n)\n",
    "Y_true=1/(1+exp(-(2+0.5*X))) # true model\n",
    "Y_noise=1/(1+exp(-(2+0.5*X+random.normal(0,1,n)))) # add noise to the truth\n",
    "Y=(Y_noise>0.5).astype(float) # transform to binary data\n",
    "\n",
    "# Plot true model and noisy data\n",
    "plot(X,Y_true,'r.')\n",
    "plot(X,Y_noise,'g.')\n",
    "plot(X,Y,'b.')\n",
    "xlabel('X')\n",
    "ylabel('Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build our neural net using a sigmoid activation function, corresponding to the logistic function. For such a classification problem, we try to minimize a loss function based on the binary cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build our model\n",
    "model_classif1 = Sequential()\n",
    "\n",
    "# Declare the layers\n",
    "layers = [Dense(units=1, input_dim=1), Activation('sigmoid')]\n",
    " \n",
    "# Add the layers to the model\n",
    "for layer in layers:\n",
    "    model_classif1.add(layer)\n",
    "    \n",
    "# Configure an optimizer used to minimize the loss function\n",
    "sgd = SGD(lr=0.1, decay=.01)\n",
    "\n",
    "# Compile our model\n",
    "model_classif1.compile(loss='binary_crossentropy', optimizer=sgd)\n",
    " \n",
    "# Fit the model\n",
    "model_classif1.fit(X, Y, validation_split=0.2, epochs=10)\n",
    "\n",
    "# Model weights\n",
    "model_classif1.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated weights can be far from the true parameters ($0.5$ and $2$). To improve the estimation, we can increase the database (*n* in the above code) and the number of iterations to fit the model (*epochs* in the *fit* function). Then, we plot the true (red) and estimated (green) classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction using the model\n",
    "Y_hat = model_classif1.predict(X)\n",
    "\n",
    "# Plot noisy data, true model and prediction\n",
    "plot(X,Y,'b.')\n",
    "plot(X,Y_true,'r.')\n",
    "plot(X,Y_hat,'g.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us consider a more complicated classification model where we have 2 input variables ($X_0$, $X_1$) and a binary output variable $Y$. The link between the $X$ variables and $Y$ is nonlinear (moon shape)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import machine learning library\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Generate data\n",
    "n=1000\n",
    "X_true,Y_true=make_moons(noise=0.00001,random_state=0,n_samples=n) # true model\n",
    "X,Y=make_moons(noise=0.2,random_state=0,n_samples=n) # noisy data\n",
    "\n",
    "# Plot true model and noisy data\n",
    "scatter(X[:,0], X[:,1], c=Y)\n",
    "scatter(X_true[:,0], X_true[:,1], c=Y_true)\n",
    "xlabel('X_0')\n",
    "ylabel('X_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try first to adjust a simple neural net with 1 hidden layer, 1 neurone and a logistic loss function. Then, test \n",
    "a more complex network with 3 hidden layers and relu (rectified linear unit) activations. For more information concerning the different activation functions, please see https://fr.wikipedia.org/wiki/Fonction_d'activation#cite_note-7. We stock the fitted model in the variable *history*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build our model\n",
    "model_classif2 = Sequential()\n",
    " \n",
    "# Declare the layers\n",
    "layers = [Dense(units=1, input_dim=2), Activation('sigmoid')] # try this one first\n",
    "#layers = [Dense(units=4, input_dim=2), Activation('relu'), # then this one\n",
    "#          Dense(units=4, input_dim=4), Activation('relu'),\n",
    "#          Dense(units=1, input_dim=4), Activation('sigmoid')]\n",
    "\n",
    "# Add the layers to the model\n",
    "for layer in layers:\n",
    "    model_classif2.add(layer)\n",
    "    \n",
    "# Configure an optimizer used to minimize the loss function\n",
    "sgd = SGD(lr=0.1, decay=.01) # try this one first\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "# Compile our model\n",
    "model_classif2.compile(loss='binary_crossentropy', optimizer=adam) # sgd or adam\n",
    " \n",
    "# Fit the model\n",
    "history = model_classif2.fit(X, Y, validation_split=0.2, epochs=1000, verbose=0)\n",
    "\n",
    "# Model weights\n",
    "model_classif2.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction using the model\n",
    "Y_hat = model_classif2.predict(X)\n",
    "\n",
    "# Plot noisy data, true model and prediction\n",
    "subplot(2,2,1)\n",
    "scatter(X[:,0], X[:,1], c=Y)\n",
    "scatter(X_true[:,0], X_true[:,1], c=Y_true)\n",
    "title('Noisy data and true model')\n",
    "subplot(2,2,2)\n",
    "scatter(X[:,0], X[:,1], c=Y_hat[:,0])\n",
    "scatter(X_true[:,0], X_true[:,1], c=Y_true)\n",
    "title('Noisy data and predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted values (right) are given between $0$ (blue) and $1$ (red). The algorithm converges when the classification boundary is well defined between the 2 scatter plots. Finally, we can use the *history* variable and plot the evolution of the loss function for the training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize history for loss\n",
    "plot(history.history['loss'])\n",
    "plot(history.history['val_loss'])\n",
    "ylabel('loss')\n",
    "xlabel('epoch')\n",
    "legend(['train', 'test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification problem on real data**\n",
    "\n",
    "Congratulations, you are now ready to apply regression or classification problems on real data. Here, we propose to use a well known dataset in statistic: *iris*. The output variable $y$ are different iris species (Setosa, Versicolour, Virginica) and input variables $x$ are the length/width of the sepal/petal. Try to construct your own neural net, learn it on a training dataset ($x_{train}$ and $x_{train}$, 80%) and test it on a independant test dataset ($x_{test}$ and $y_{test}$, 20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "description": "Use Caffe as a generic SGD optimizer to train logistic regression on non-image HDF5 data.",
  "example_name": "Off-the-shelf SGD for classification",
  "include_in_docs": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "priority": 3
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
